<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="CAST (Counterfactual Augmentation with Synthetic Trajectories) is a method for augmenting uncurated robot datasets with counterfactual trajectories to improve their ability to be steered by language instructions.">
    <meta name="keywords" content="CAST, cast, language, robotics, navigation, counterfactuals">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Counterfactual Augmentation with Synthetic Trajectories</title>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZYH3N96LN5"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'G-ZYH3N96LN5');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/slick.css">
    <link rel="stylesheet" href="./static/css/slick-theme.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/slick.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-2 publication-title">Counterfactual Labels Improve Instruction
                            Following in Vision-Language-Action Models</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://catglossop.github.io">Catherine Glossop</a>,</span>
                            <span class="author-block">
                                <a href="https://x.com/verityw_">William Chen</a>,</span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=AGPooMYAAAAJ&hl=en">Arjun Bhorkar</a>,</span>
                            <span class="author-block">
                                <a href="https://robodhruv.github.io/">Dhruv Shah</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
                            </span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- Talk Link. -->
                                <span class="link-block">
                                    <a href="https://youtu.be/ADGGmoCHuI0"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-youtube"></i>
                                        </span>
                                        <span>Video</span>
                                    </a>
                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/catglossop/CAST"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github-alt"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                                <!-- Dataset Link. -->
                                <span class="link-block">
                                    <a href="https://huggingface.co/datasets/catglossop/CAST-dataset"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-database"></i>
                                        </span>
                                        <span>Data</span>
                                    </a>
                                </span>
                                <!-- ArXiV Link -->
                                <span class="link-block">
                                    <a href="TODO"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file"></i> 
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>

                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop is-centered has-text-justified is-size-5">
            <div class="hero-body">
                <center><video width="640" height="480" id="teaser" autoplay muted loop playsinline fetchpriority="high" controls>
                    <source src="./static/videos/cast_teaser_32.mp4" type="video/mp4">
                </video></center>
                <center><h2 class="title" style="margin-bottom: 20px">Abstract</h2></center>
                <p> </p>
                <p>
                    Generalist robots should be able to understand and follow user instructions, but current vision-language-action (VLA) models struggle with following fine-grained commands despite providing a powerful architecture for mapping open-vocabulary natural language instructions to robot actions. We attribute this phenomenon to a lack of semantic diversity and language grounding in existing robot datasets and, specifically, a lack of fine-grained <i>task</i> diversity for similar observations. To address this, we present a novel method to <i>augment</i> existing robot datasets by leveraging vision language models to create <i>counterfactual</i> labels. Our method improves the language-following capabilities of VLAs by increasing the diversity and granularity of language grounding for robot datasets by generating counterfactual language and actions.
                </p>
            </div>
        </div>
    </section>

    <!-- Approach -->
    <section class="hero is-small">
        <div class="hero-body">
            <center><h1 class="title" style="margin-bottom: 20px">Approach</h1></center>
            <div class="container is-max-desktop is-centered has-text-justified is-size-6">
                <h2 class="subtitle"><b>Counterfactual Augmentation with Synthetic Trajectories (CAST)</b></h2>
                <p class="body">We propose CAST, <b>C</b>ounterfactual <b>A</b>ugmentation with <b>S</b>ynthetic <b>T</b>rajectories, a method for both expanding and balancing the distribution of trajectories in your robotics dataset.</p>
            </div>
        </div>
        <center><img width=640 height=480 src="./static/images/cast_birdseye_diagram.png" alt="CAST Method" class="cast-diagram"></center>
        <div class="hero-body">
            <div class="container is-max-desktop is-centered has-text-justified is-size-6">
                <p>We observed that often VLA policies will learn to ignore language, especially when training on sufficiently diverse language instructions, as biases in the data result in the policy only needing to pay attention to the observation to predict the correct actions during training. Therefore, the goal of CAST is to expand the distribution of instructions and actions at any given observation. We show that this forces the model to attend to language, producing a more steerable policy.</p>
            </div>
        </div>
        <center><video width="640" id="teaser" autoplay muted loop playsinline fetchpriority="high" controls>
            <source src="./static/videos/cast_method_overview_32.mp4" type="video/mp4">
        </video></center>
        <div class="hero-body">    
            <div class="container is-max-desktop is-centered has-text-justified is-size-6">
                <p>CAST takes an uncurated robot dataset queries a VLM for counterfactual instructions that could have been executed at different points in the existing trajectories. The atomic command that aligns with the instruction is also determined and used to generate an action chunk. We can then append these counterfactual actions to the existing trajectory, creating the CAST dataset, which consists of both the original data and these counterfactual trajectory-instruction pairs. We use the CAST dataset to finetune PaliGemma-3B with the typical VLA recipe.</p>
            </div>
        </div>
    </section>

    <!-- Results Carousel -->
    <section class="hero is-light is-small">
        <center><h1 class="title" style="text-align: center; margin: bottom 20px;">Experiments</h1></center>
        <!-- Object Navigation -->
        <div class="hero-body">
            <h2 class="subtitle" style="text-align: center; margin: bottom 20px;">Object Navigation</h2>
            <div class="container is-max-desktop is-centered has-text-justified is-size-6">
            
                <p class="body">In object navigation tasks, the policy must recognize and move towards a target object. CAST increases the coverage of object reaching behavior in the general navigation dataset we train CounterfactualVLA on, resulting in successful object navigation.</p>
                <div id="results-carousel" class="carousel results-carousel">
                    <div class="item item-cone">
                        <video poster="" id="cone" autoplay controls muted loop playsinline height="100%">
                            <source src="static/videos/vfw_go_to_person_32.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-stairs">
                        <video poster="" id="stairs" autoplay controls muted loop playsinline height="100%">
                            <source src="static/videos/vfw_move_to_stairs_32.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-purple-cushion">
                        <video poster="" id="purple-cushion" autoplay controls muted loop playsinline height="100%">
                            <source src="static/videos/vfw_move_tree_left_32.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
            </div>
        </div>

        <!-- Referential Navigation -->
        <div class="hero-body">
            <h2 class="subtitle" style="text-align: center; margin: bottom 20px;">Referential Navigation</h2>
            <div class="container is-max-desktop is-centered has-text-justified is-size-6">
            
            <p class="body">In referential navigation tasks, the policy must recognize a target object or structure in the scene and understand how to move with reference to that object, specifically moving to the "right" or "left". By explicitly generating counterfactual trajectories that reflect these referential instructions, CounterfactualVLA learns to differentiate these behaviors and perform them accordingly </p>
            <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                    <div class="item item-cone">
                        <video poster="" id="cone" autoplay controls muted loop playsinline height="100%">
                            <source src="static/videos/vfw_left_pillar_32.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-stairs">
                        <video poster="" id="stairs" autoplay controls muted loop playsinline height="100%">
                            <source src="static/videos/vfw_right_pillar_32.mp4" type="video/mp4">
                        </video>
                            <source src="static/videos/vfw_move_tree_left_32.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-purple-cushion">
                        <video poster="" id="purple-cushion" autoplay controls muted loop playsinline height="100%">
                            <source src="static/videos/vfw_follow_right_wall_32.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
            </div>
        </div>
    
        <!-- Continuous Navigation -->
        <div class="hero-body">
            <div class="container is-max-desktop is-centered has-text-justified is-size-6">
            <p class="body">In continuous navigation tasks, the policy must recognize a target structure and move with respect to the structure for a sustained amount of time. CouterfactualVLA demonstrates the ability to perform these continuous behaviors in scenes where the training data is typically biased to moving down the center of hallways or sticking to more open areas.</p>
            <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                    <div class="item item-cone">
                        <video poster="" id="cone" autoplay controls muted loop playsinline height="100%">
                            <source src="static/videos/vfw_move_along_bushes_32.mp4" type="video/mp4">    
                        </video>
                    </div>
                    <div class="item item-stairs">
                        <video poster="" id="stairs" autoplay controls muted loop playsinline height="100%">
                            <source src="static/videos/vfw_move_along_white_wall_32.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-purple-cushion">
                        <video poster="" id="purple-cushion" autoplay controls muted loop playsinline height="100%">
                            <source src="static/videos/vfw_follow_right_wall_32.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
            </div>
        </div>

        <!-- Chained Instruction Videos -->
        <div class="hero-body">
            <h2 class="subtitle" style="text-align: center; margin: bottom 20px;">Chained Instructions</h2>
            <div class="container is-max-desktop is-centered has-text-justified is-size-6">
            <p class="body">In these examples, the policy is provided instructions in sequence. It must execute each step correctly to move on to the next step.</p>
            <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                    <div class="item item-cone">
                        <video poster="" id="cone" autoplay controls muted loop playsinline height="100%">
                            <source src="static/videos/cast_chain_indoor_w_speed_32.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-stairs">
                        <video poster="" id="stairs" autoplay controls muted loop playsinline height="100%">
                            <source src="static/videos/cast_chain_outside_w_speed_32.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Results -->
    <section class="hero is-small">
        <div class="hero-body">
            <center><h1 class="title" style="margin-bottom: 20px">Results</h1></center>
            <div class="container is-max-desktop is-centered has-text-justified is-size-6">
                <p class="body">By training CounterfactualVLA with the CAST dataset, we can match and outperform state-of-the-art methods for open-set language following.</p>
            </div>
        </div>
        <center><img width=800 src="./static/images/results_w_standard.png" alt="CAST Method" class="cast-diagram"></center>
        <div class="hero-body">
            <div class="container is-max-desktop is-centered has-text-justified is-size-6">
                <p class="body">We also compare to alternative architectures, which demonstrates that having a high-capacity model in necessary to fully reap the benefits of diverse language labels.</p>
            </div>
        </div>
        <center><img width=480 src="./static/images/results_architectures.png" alt="CAST Method" class="cast-diagram"></center>
    </section>


    <section class="section" id="BibTeX">
            <pre><code>@article{glossop2025cast,
                    author  = {Catherine Glossop and William Chen and Arjun Bhorkar and Dhruv Shah and Sergey Levine},
                    title   = {{Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models}},
                    journal = {arXiv pre-print},
                    year    = {2023},
                    url     = {https://arxiv.org/abs/2310.07896}
                  }</code></pre>
        </div>
    </section>

    <br>
    <center class="is-size-10">
      The website (<a href="https://github.com/catglossop/counterfactual-augmentation-w-synthetic-trajectories">source code</a>) design was adapted from <a href="https://nerfies.github.io" class="external-link"><span
                class="dnerf">Nerfies</span></a>.
    </center>
    <br>
</body>

</html>
